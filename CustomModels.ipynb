{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b24430f",
   "metadata": {},
   "source": [
    "# 0. Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "204c44a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as sts\n",
    "from scipy.spatial.distance import cdist\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.datasets import make_regression, make_classification\n",
    "from sklearn.base import BaseEstimator\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, mean_squared_error\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1578d034",
   "metadata": {},
   "source": [
    "# 0.1 Batch Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "0ea8b17e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, y, shuffle_=True, batch_size=1):\n",
    "    \"\"\"\n",
    "    Batch generator\n",
    "    X          - features matrix\n",
    "    y_batch    - answers vector\n",
    "    shuffle    - is it necessary to randomly shuffle the selection\n",
    "    batch_size - batch ( 1 for SGD, > 1 for mini-batch GD)\n",
    "    Generates subsample for an interation of descent (X_batch, y_batch)\n",
    "    \"\"\"\n",
    "    n = X.shape[0]\n",
    "    if shuffle_:\n",
    "        X, y = shuffle(X, y)\n",
    "    for i in range(0, n, batch_size):\n",
    "        try:\n",
    "            X_batch, y_batch = X[i:i + batch_size], y[i:i + batch_size]\n",
    "        except IndexError:\n",
    "            X_batch, y_batch = X[i::], y[i::]\n",
    "        yield (X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "801f1547",
   "metadata": {},
   "source": [
    "# 1. k-Nearest neighbors classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "fbc1f1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyKNeighborsClassifier(BaseEstimator):\n",
    "    \"\"\"kNN classifier as in sklearn.\n",
    "        n_neighbors -> int,\n",
    "        metric -> {‘canberra’, \n",
    "        ‘chebyshev’, \n",
    "        ‘cityblock’, \n",
    "        ‘correlation’, \n",
    "        ‘cosine’, \n",
    "        ‘dice’,\n",
    "        ‘euclidean’,\n",
    "        ‘hamming’,\n",
    "        ‘jaccard’,\n",
    "        ‘jensenshannon’, \n",
    "        ‘kulczynski1’,\n",
    "        ‘mahalanobis’, \n",
    "        ‘matching’, \n",
    "        ‘minkowski’,\n",
    "        ‘rogerstanimoto’, \n",
    "        ‘russellrao’,\n",
    "        ‘seuclidean’, \n",
    "        ‘sokalmichener’, \n",
    "        ‘sokalsneath’, \n",
    "        ‘sqeuclidean’,\n",
    "        ‘yule’}\n",
    "    \"\"\"\n",
    "    def __init__(self, n_neighbors, algorithm='brute', metric = 'euclidean'):\n",
    "        self.n_neighbors = n_neighbors\n",
    "        self.metric = metric\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        self.X_train = X\n",
    "        self.y_train = y\n",
    "    \n",
    "    def predict(self, X):\n",
    "        distance_matrix = cdist(X, self.X_train, metric = self.metric)\n",
    "        ind_matrix = np.argsort(distance_matrix, axis = 1)[:, :self.n_neighbors]\n",
    "        result = [np.argmax(np.bincount(self.y_train[ind_matrix][k])) for k in range(len(ind_matrix))]\n",
    "        return np.array(result)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8ab9613",
   "metadata": {},
   "source": [
    "# 1.1 Test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "051f6fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris = datasets.load_iris()\n",
    "X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=0.1, stratify=iris.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "a5674c02",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = KNeighborsClassifier(n_neighbors=2, algorithm='brute')\n",
    "my_clf = MyKNeighborsClassifier(n_neighbors=2, algorithm='brute', metric = 'cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "61858578",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)\n",
    "my_clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "y_mypred = my_clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "c467565d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My f1-score = 1.0\n",
      "Scikit-learn f1-score = 0.9326599326599326\n"
     ]
    }
   ],
   "source": [
    "print(f\"My f1-score = {f1_score(y_test, y_mypred, average='macro')}\")\n",
    "print(f\"Scikit-learn f1-score = {f1_score(y_test, y_pred, average='macro')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f652e0e2",
   "metadata": {},
   "source": [
    "# 2. Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "031d6caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLinearRegression():\n",
    "    def __init__(self, alpha=0.001, n_iters=1000):\n",
    "        self.weights = None\n",
    "        self.alpha = alpha\n",
    "        self.n_iters = n_iters\n",
    "        \n",
    "    def gradient_step(self, weights, weights_grad):\n",
    "        return weights - self._alpha * weights_grad\n",
    "             \n",
    "            \n",
    "    def grad_func(self, X_batch, y_batch, B):\n",
    "        return 2 * X_batch.T.dot(np.dot(X_batch, self.weights) - y_batch) / B\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        B = 1\n",
    "        n, m = X.shape\n",
    "        self.weights = np.random.normal(size=(m+1,))\n",
    "        ones_X = np.concatenate((np.ones((n, 1)), X), axis=1)\n",
    "        for i in range(self.n_iters):\n",
    "            for X_batch, y_batch in batch_generator(ones_X, y, batch_size=5):\n",
    "                grad = self.grad_func(X_batch, y_batch, B) \n",
    "                self.weights -= self.alpha * grad\n",
    "                \n",
    "    def predict(self, X):\n",
    "        n, m = X.shape\n",
    "        ones_X = np.concatenate((np.ones((n, 1)), X), axis=1)\n",
    "        return np.dot(ones_X, self.weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf42aaf",
   "metadata": {},
   "source": [
    "# 2.1 Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "387decef",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, coef = make_regression(n_samples = 10_000, n_features=5, n_informative = 5, noise = 0.4, coef=True, bias = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "dfdea73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "ea8a3ff8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [82.81040567 63.14581477 41.10830795 91.81265587 69.7716958 ]\n",
      "b = 10.003370547754722\n",
      "MSE = 0.164167\n"
     ]
    }
   ],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(X_train, y_train)\n",
    "reg_pred = reg.predict(X_test)\n",
    "print(f\"w = {reg.coef_}\\nb = {reg.intercept_}\")\n",
    "print(f\"MSE = {mean_squared_error(reg_pred, y_test):.6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "e9bc9140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [ 9.97575072 82.82760533 63.14873993 41.11977718 91.81230787 69.75720842]\n",
      "\n",
      "MSE = 0.164221\n"
     ]
    }
   ],
   "source": [
    "myreg = MyLinearRegression()\n",
    "myreg.fit(X_train, y_train)\n",
    "reg_pred = myreg.predict(X_test)\n",
    "print(f\"w = {myreg.weights}\\n\")\n",
    "print(f\"MSE = {mean_squared_error(reg_pred, y_test):.6}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c292abd2",
   "metadata": {},
   "source": [
    "# 3 Logisctic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "id": "308addec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLogisticRegression():\n",
    "    def __init__(self, alpha=0.001, n_iters=1000):\n",
    "        self.weights = None\n",
    "        self.alpha = alpha\n",
    "        self.n_iters = n_iters\n",
    "        \n",
    "    def gradient_step(self, weights, weights_grad):\n",
    "        return weights - self._alpha * weights_grad\n",
    "          \n",
    "            \n",
    "    def grad_func(self, X_batch, y_batch, B):\n",
    "        return 2 * X_batch.T.dot(self.sigmoid(np.dot(X_batch, self.weights)) - y_batch)/ B\n",
    "    \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        B = 2\n",
    "        n, m = X.shape\n",
    "        self.weights = np.random.normal(size=(m+1,))\n",
    "        ones_X = np.concatenate((np.ones((n, 1)), X), axis=1)\n",
    "        for i in range(self.n_iters):\n",
    "            for X_batch, y_batch in batch_generator(ones_X, y, batch_size=B):\n",
    "                grad = self.grad_func(X_batch, y_batch, B) \n",
    "                self.weights -= self.alpha * grad\n",
    "      \n",
    "    def sigmoid(self, X):  \n",
    "        return 1. / (1. + np.exp(-X))\n",
    "    \n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        return self.sigmoid(np.dot(X, self.weights))\n",
    "    \n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        n, m = X.shape\n",
    "        ones_X = np.concatenate((np.ones((n, 1)), X), axis=1)\n",
    "        y_pred = self.predict_proba(ones_X) > threshold\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a898001",
   "metadata": {},
   "source": [
    "# 3.1 Тест"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "0a68d3b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples = 10_000, n_features=3, n_redundant=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "id": "19ae2046",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "id": "da5d235e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[-2.93248428e-03 -1.32504511e-02  3.19619315e+00]]\n",
      "b = [-0.09591955]\n",
      "f1-score = 0.914767\n"
     ]
    }
   ],
   "source": [
    "clf = LogisticRegression()\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "print(f\"w = {clf.coef_}\\nb = {clf.intercept_}\")\n",
    "print(f\"f1-score = {f1_score(y_pred, y_test):.6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "id": "f4e035c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [-0.09653199 -0.00454937 -0.01407133  3.21411689]\n",
      "\n",
      "f1-score = 0.914767\n"
     ]
    }
   ],
   "source": [
    "myclf = MyLogisticRegression()\n",
    "myclf.fit(X_train, y_train)\n",
    "y_pred = myclf.predict(X_test)\n",
    "print(f\"w = {myclf.weights}\\n\")\n",
    "print(f\"f1-score = {f1_score(y_pred, y_test):.6}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a18a1ebe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d8782da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6311d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "095dce52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyTorch_1_5v2",
   "language": "python",
   "name": "pytorch_1_5v2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
